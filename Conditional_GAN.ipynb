{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataDir = \"/Users/test/Desktop/Master_Thesis_Flow/real-nvp/data/GPR_Data/B200/B200 Pictures HR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CropTrainingData(DataPath):\n",
    "    \n",
    "    # Storing Images\n",
    "    Cropped = []\n",
    "    \n",
    "    # Iteration over all0 Image Files\n",
    "    for Img in tqdm(os.listdir(DataPath)):\n",
    "        # Reading File Path with Error Handling\n",
    "        try:\n",
    "            ImgArray = cv2.imread(os.path.join(DataPath,Img),cv2.IMREAD_GRAYSCALE)\n",
    "            # Append Image Snippets to Storing Variable\n",
    "            if ImgArray.shape == (128,128):\n",
    "                Cropped.append(ImgArray[0:128,0:128])\n",
    "                Cropped.append(ImgArray[0:128,124:252])\n",
    "                Cropped.append(ImgArray[0:128,248:376])\n",
    "                Cropped.append(ImgArray[0:128,372:500])\n",
    "        except:\n",
    "            pass\n",
    "    # convert to Numpy Array and add Dimension like in PyTorch Datasets        \n",
    "    return np.expand_dims(np.array(Cropped).astype(object),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 608/608 [00:01<00:00, 361.78it/s]\n"
     ]
    }
   ],
   "source": [
    "TrainingData = CropTrainingData(DataDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(TrainingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_parameters = {\n",
    "    \"n_epochs\": 100,\n",
    "    \"batch_size\": 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a619d7630de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGPRDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class GPRDataset(Dataset):\n",
    "    def __init__ (self, img_file, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_file = img_file\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(img_file)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "            \n",
    "        img_name = os.path.join(self.root_dir, self.img_file[idx])\n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "         #image = image[0:128,0:128]\n",
    "            \n",
    "        return image \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataset = GPRDataset(DataDir)\n",
    "    dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(i, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
